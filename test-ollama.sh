#!/bin/bash

# Test script for Ollama integration with llimp
# This script demonstrates how to use the macro with a local Ollama instance

set -e

echo "ü¶ô LLImp + Ollama Test Script"
echo "====================================="

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "‚ùå Ollama is not installed. Please install it from https://ollama.com"
    exit 1
fi

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags &> /dev/null; then
    echo "‚ùå Ollama is not running. Please start it with: ollama serve"
    exit 1
fi

echo "‚úÖ Ollama is running"

# Check if we have any models
MODELS=$(curl -s http://localhost:11434/api/tags | jq -r '.models[].name' 2>/dev/null || echo "")

if [ -z "$MODELS" ]; then
    echo "‚ö†Ô∏è  No models found. Pulling gemma3..."
    ollama pull gemma3:latest
    echo "‚úÖ Model downloaded"
else
    echo "‚úÖ Available models:"
    echo "$MODELS" | sed 's/^/   - /'
fi

# Set environment variables for Ollama
export LLM_BASE_URL=http://localhost:11434/v1
unset LLM_API_KEY  # Not needed for localhost

# You can also use remote Ollama servers:
# export OLLAMA_HOST=192.168.1.100  # Remote server IP
# export OLLAMA_HOST=ollama.example.com  # Remote hostname
# unset LLM_BASE_URL  # Let OLLAMA_HOST take effect

echo ""
echo "üîß Configuration:"
echo "   LLM_BASE_URL=$LLM_BASE_URL"
if [ -n "$OLLAMA_HOST" ]; then
    echo "   OLLAMA_HOST=$OLLAMA_HOST (will override default localhost)"
else
    echo "   OLLAMA_HOST: <not set> (using localhost)"
fi
echo "   LLM_API_KEY: <not set> (not needed for Ollama)"

echo ""
echo "üöÄ Building calculator example with Ollama..."

# Build the calculator example
if cargo build -p calculator --quiet; then
    echo "‚úÖ Build successful!"
else
    echo "‚ùå Build failed. This is expected if no cached implementations exist and the model fails to generate code."
    echo "   Try running the example to generate implementations first."
fi

echo ""
echo "üß™ Testing with a simple example..."

# Create a temporary test file
cat > /tmp/ollama_test.rs << 'EOF'
use openai_macro::openai_impl;

trait SimpleTest {
    fn hello(&self) -> String;
}

struct Test;

#[llimp(
    model = "gemma3",
    prompt = "Return a simple hello message"
)]
impl SimpleTest for Test {
    fn hello(&self) -> String {
        // Generated by LLM
    }
}

fn main() {
    let test = Test;
    println!("{}", test.hello());
}
EOF

echo "üìù Created simple test file"

# Test if we can at least parse the file (compilation will fail without implementations)
echo ""
echo "üìä Cache directory contents:"
if [ -d "target/llimp/llimp_cache" ]; then
    ls -la target/llimp/llimp_cache/ | head -10
    CACHE_COUNT=$(ls target/llimp/llimp_cache/ | wc -l)
    echo "   Total cached implementations: $CACHE_COUNT"
else
    echo "   No cache directory found"
fi

echo ""
echo "‚ú® LLImp + Ollama integration is ready!"
echo ""
echo "üí° Next steps:"
echo "   Local Ollama:"
echo "   1. Run: export LLM_BASE_URL=http://localhost:11434/v1"
echo "   2. Build your project: cargo build"
echo ""
echo "   Remote Ollama:"
echo "   1. Run: export OLLAMA_HOST=192.168.1.100  # Your remote server"
echo "   2. Build your project: cargo build"
echo ""
echo "   3. Use any model available in Ollama: gemma3, deepseek-coder, llama3.1, etc."
echo "   4. The first build will be slow as implementations are generated"
echo "   5. Subsequent builds use cached implementations and are fast"
echo ""
echo "üéØ To test with the calculator example:"
echo "   Local:  export LLM_BASE_URL=http://localhost:11434/v1 && cargo run -p calculator"
echo "   Remote: export OLLAMA_HOST=your-server-ip && cargo run -p calculator"

# Cleanup
rm -f /tmp/ollama_test.rs

//! # LLImp (Large Language Implementation)
//!
//! A procedural macro that generates Rust code using LLMs (Large Language Models).
//!
//! ## Usage
//!
//! ```rust,ignore
//! use llimp::llimp;
//!
//! trait Calculator {
//!     fn add(&self, a: i32, b: i32) -> i32;
//!     fn multiply(&self, a: i32, b: i32) -> i32;
//! }
//!
//! struct MyCalculator;
//!
//! #[llimp(model = "gemini-2.5-flash", prompt = "Implement basic arithmetic operations")]
//! impl Calculator for MyCalculator {}
//! ```
//!
//! The macro will call the LLM API to generate the method implementations based on:
//! - The trait signatures
//! - The struct type
//! - Any additional prompt hints you provide
//!
//! ## Configuration
//!
//! By default, llimp uses Ollama (local LLM server) at `http://localhost:11434/v1`.
//!
//! ### Environment Variables
//!
//! - `LLM_MODEL` - Model to use (default: "gemma3:latest")
//! - `LLM_API_KEY` - API key for cloud services (not needed for Ollama)
//! - `LLM_BASE_URL` - Override endpoint URL
//! - `OLLAMA_HOST` - Ollama server hostname/IP (default: "localhost")
//!
//! ## Ollama Support (Default)
//!
//! By default, llimp uses Ollama running locally:
//! ```bash
//! ollama serve
//! ollama pull gemma3:latest  # or your preferred coding model
//! export LLM_MODEL=gemma3:latest  # optional, this is the default
//! cargo run
//! ```
//!
//! For remote Ollama servers:
//! ```bash
//! export OLLAMA_HOST=192.168.1.100  # or your remote server IP/hostname
//! cargo run
//! ```
//!
//! ## Cloud API Override
//!
//! To use Google Gemini or other cloud APIs instead:
//! ```bash
//! export LLM_MODEL=gemini-2.0-flash-exp
//! export LLM_API_KEY=your_api_key
//! export LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai
//! cargo run
//! ```
//!
//! ## Caching
//!
//! Generated implementations are cached based on a hash of the input parameters.
//! The cache is stored in your build output directory.
//!
//! ## Offline Mode
//!
//! Set `LLM_OFFLINE=1` or enable the `no-network` feature to use only cached implementations
//! and avoid network requests.
//!
//! ## Features
//!
//! - `no-network`: Disables network requests and uses only cached implementations

use proc_macro::TokenStream;
use quote::{quote, ToTokens};
use sha2::{Digest, Sha256};
use std::{env, fs, path::PathBuf};
use syn::{parse_macro_input, punctuated::Punctuated, ItemImpl, Lit, Meta, MetaNameValue, Token};

/// A procedural macro that generates Rust method implementations using OpenAI's API.
///
/// This macro takes an `impl` block with empty method bodies and generates implementations
/// by calling OpenAI's API with the method signatures and any provided hints.
///
/// ## Parameters
///
/// - `prompt` (optional): Additional context or instructions for the AI
///
/// Note: The model is specified via the `LLM_MODEL` environment variable.
///
/// ## Example
///
/// ```rust,ignore
/// use llimp::llimp;
///
/// trait Greeter {
///     fn greet(&self, name: &str) -> String;
///     fn farewell(&self, name: &str) -> String;
/// }
///
/// struct FriendlyGreeter;
///
/// #[llimp(prompt = "Be friendly and enthusiastic in responses")]
/// impl Greeter for FriendlyGreeter {
///     fn greet(&self, name: &str) -> String {
///         // Implementation will be generated by LLM
///     }
///
///     fn farewell(&self, name: &str) -> String {
///         // Implementation will be generated by OpenAI
///     }
/// }
/// ```
#[proc_macro_attribute]
pub fn llimp(args: TokenStream, input: TokenStream) -> TokenStream {
    let args = parse_macro_input!(args with Punctuated::<Meta, Token![,]>::parse_terminated);
    let mut prompt_hint: Option<String> = None;

    for arg in args {
        match arg {
            Meta::NameValue(MetaNameValue {
                path,
                value:
                    syn::Expr::Lit(syn::ExprLit {
                        lit: Lit::Str(val), ..
                    }),
                ..
            }) => {
                let key = path.to_token_stream().to_string();
                match key.as_str() {
                    "prompt" => prompt_hint = Some(val.value()),
                    _ => {}
                }
            }
            _ => {}
        }
    }

    let item_impl = parse_macro_input!(input as ItemImpl);
    // We render the original impl header but replace fn bodies.
    let trait_path = item_impl
        .trait_
        .as_ref()
        .map(|(_, p, _)| p.to_token_stream().to_string());
    let self_ty = item_impl.self_ty.to_token_stream().to_string();

    // Collect method signatures for the prompt.
    let mut sigs = Vec::new();
    for item in &item_impl.items {
        if let syn::ImplItem::Fn(f) = item {
            sigs.push(f.sig.to_token_stream().to_string());
        }
    }

    // Build a deterministic cache key including model.
    let model = env::var("LLM_MODEL").unwrap_or_else(|_| "gemma3:latest".to_string());
    let prompt_seed = format!(
        "trait={:?}\nself_ty={}\nmethods={:#?}\nprompt_hint={:?}\nmodel={}",
        trait_path, self_ty, sigs, prompt_hint, model
    );
    let mut hasher = Sha256::new();
    hasher.update(prompt_seed.as_bytes());
    let key_hex = hex::encode(hasher.finalize());

    // Prepare OUT_DIR cache path
    let out_dir = env::var("OUT_DIR")
        .ok()
        .map(PathBuf::from)
        .unwrap_or_else(|| PathBuf::from("./target/llimp"));
    let cache_dir = out_dir.join("llimp_cache");
    let _ = fs::create_dir_all(&cache_dir);
    let cache_file = cache_dir.join(format!("{key_hex}.rs"));

    // Decide generation strategy.
    let offline =
        env::var("LLM_OFFLINE").ok().as_deref() == Some("1") || cfg!(feature = "no-network");

    let generated_impl = if offline && cache_file.exists() {
        fs::read_to_string(&cache_file).expect("read cache")
    } else {
        let mdl = env::var("LLM_MODEL").unwrap_or_else(|_| "gemma3:latest".to_string());
        let sys = r#"Return JSON array of Rust function bodies.

Input: function signatures
Output: ["{ rust_code }", "{ rust_code }"]

ONLY return the JSON array. No explanation."#;

        let human = format!(
            r#"{}

{}

Return exactly {} function bodies as JSON array."#,
            sigs.join("\n"),
            prompt_hint.unwrap_or("".to_string()),
            sigs.len(),
        );
        let code = call_llm(&mdl, &sys, &human).unwrap_or_else(|e| panic!("llimp error: {e:?}"));

        // Basic sanity: we expect one body per method, delimited.
        // Easiest contract: the model returns a JSON array of strings,
        // each string is the body for the corresponding method in order.
        // (We instruct this contract in `call_llm`.)
        let bodies: Vec<String> = serde_json::from_str(&code)
            .unwrap_or_else(|_| vec![code])
            .into_iter()
            .map(|body| unescape_json_string(&body))
            .collect(); // fallback: single blob

        // Stitch methods back into an impl block.
        synthesize_impl(&item_impl, &bodies)
            .unwrap_or_else(|e| panic!("synthesis error: {e}"))
            .to_string()
    };

    // Cache
    let _ = fs::write(&cache_file, &generated_impl);

    // Return tokens from the generated string.
    generated_impl.parse().unwrap()
}

fn synthesize_impl(
    item_impl: &ItemImpl,
    bodies: &[String],
) -> anyhow::Result<proc_macro2::TokenStream> {
    use anyhow::{bail, Context};

    // Rebuild the impl header
    let unsafety = &item_impl.unsafety;
    let impl_token = &item_impl.impl_token;
    let generics = &item_impl.generics;
    let self_ty = &item_impl.self_ty;

    // Handle trait implementation properly
    let trait_impl = if let Some((_, trait_path, _)) = &item_impl.trait_ {
        quote! { #trait_path for }
    } else {
        quote! {}
    };

    // Replace each fn body with a parsed body from `bodies` in order.
    let mut items = Vec::new();
    let mut body_iter = bodies.iter();
    for item in &item_impl.items {
        match item {
            syn::ImplItem::Fn(f) => {
                let mut f = f.clone();
                let body_src = body_iter
                    .next()
                    .with_context(|| "not enough bodies returned")?;

                // Parse `{ ... }` from the returned string; if the string didn't include braces, wrap it.
                let wrapped = if body_src.trim_start().starts_with('{') {
                    body_src.clone()
                } else {
                    format!("{{ {body_src} }}")
                };
                f.block = syn::parse_str::<syn::Block>(&wrapped).with_context(|| {
                    format!(
                        "failed to parse generated body into a block. Generated body was: {:?}",
                        wrapped
                    )
                })?;
                items.push(syn::ImplItem::Fn(f));
            }
            other => items.push(other.clone()),
        }
    }
    if body_iter.next().is_some() {
        // Too many bodies
        bail!("too many bodies returned compared to fn methods");
    }

    let tokens = quote! {
        #unsafety #impl_token #generics #trait_impl #self_ty #generics {
            #(#items)*
        }
    };
    Ok(tokens)
}

#[derive(serde::Serialize)]
struct ChatReq<'a> {
    model: &'a str,
    messages: Vec<Msg<'a>>,
}

#[derive(serde::Serialize)]
struct Msg<'a> {
    role: &'a str,
    content: &'a str,
}

// Unescape JSON strings that have been over-escaped by LLMs
fn unescape_json_string(s: &str) -> String {
    s.replace("\\\"", "\"")
        .replace("\\n", "\n")
        .replace("\\t", "\t")
        .replace("\\r", "\r")
        .replace("\\!", "!")
        .replace("\\\n", "\n")
        .replace("\\\\", "\\")
}

// Extract any content between braces from LLM response
fn extract_function_bodies_from_response(content: &str) -> Vec<String> {
    let mut bodies = Vec::new();
    let mut current_body = String::new();
    let mut brace_count = 0;
    let mut in_body = false;

    for ch in content.chars() {
        if ch == '{' {
            if !in_body {
                in_body = true;
                current_body.clear();
            }
            current_body.push(ch);
            brace_count += 1;
        } else if ch == '}' && in_body {
            current_body.push(ch);
            brace_count -= 1;

            if brace_count == 0 {
                // Complete body found
                bodies.push(current_body.trim().to_string());
                in_body = false;
            }
        } else if in_body {
            current_body.push(ch);
        }
    }

    bodies
}

// Returns a JSON string array of method bodies.
fn call_llm(model: &str, system: &str, user: &str) -> anyhow::Result<String> {
    use anyhow::{bail, Context};
    if cfg!(feature = "no-network") || env::var("LLM_OFFLINE").ok().as_deref() == Some("1") {
        bail!("network disabled (LLM_OFFLINE=1 or feature=no-network)");
    }

    // Default to Ollama if no API key is provided, otherwise use cloud API
    let base = env::var("LLM_BASE_URL").unwrap_or_else(|_| {
        if env::var("LLM_API_KEY")
            .map(|k| !k.is_empty())
            .unwrap_or(false)
        {
            // Cloud API mode - default to Google Gemini
            "https://generativelanguage.googleapis.com/v1beta/openai".to_string()
        } else {
            // Ollama mode - allow remote host specification
            let ollama_host = env::var("OLLAMA_HOST").unwrap_or_else(|_| "localhost".to_string());
            format!("http://{}:11434/v1", ollama_host)
        }
    });

    // For Ollama endpoints (localhost or remote), API key is optional
    let key = if base.contains("localhost") || base.contains("127.0.0.1") || base.contains(":11434")
    {
        env::var("LLM_API_KEY").unwrap_or_else(|_| "dummy".to_string())
    } else {
        env::var("LLM_API_KEY")
            .context("missing LLM_API_KEY for cloud API - set both LLM_API_KEY and LLM_BASE_URL")?
    };

    // Simple combined prompt
    let combined_prompt = format!("{}\n\n{}", system, user);

    let req = ChatReq {
        model,
        messages: vec![Msg {
            role: "user",
            content: &combined_prompt,
        }],
    };

    let client = reqwest::blocking::Client::new();
    let resp = client
        .post(format!("{base}/chat/completions"))
        .bearer_auth(key)
        .json(&req)
        .send()
        .context("request failed")?
        .error_for_status()
        .context("non-200 response")?
        .text()
        .context("read body failed")?;

    // extract JSON content
    let v: serde_json::Value = serde_json::from_str(&resp).context("parse api json")?;
    let content = v["choices"][0]["message"]["content"]
        .as_str()
        .ok_or_else(|| anyhow::anyhow!("missing content"))?;

    // Try to parse as JSON array directly (Google returns simpler format)
    if let Ok(arr) = serde_json::from_str::<serde_json::Value>(content) {
        if arr.is_array() {
            return Ok(content.to_string());
        }
    }

    // Extract any function bodies from the response (handles markdown and other formats)
    let bodies = extract_function_bodies_from_response(content);
    if !bodies.is_empty() {
        return Ok(serde_json::to_string(&bodies)?);
    }

    // Fallback: try to extract JSON from markdown code blocks or plain text
    let cleaned = content
        .lines()
        .map(|line| line.trim())
        .filter(|line| !line.starts_with("```"))
        .collect::<Vec<_>>()
        .join("\n");

    if let Ok(_) = serde_json::from_str::<serde_json::Value>(&cleaned) {
        Ok(cleaned)
    } else {
        // Last resort: return as single item array
        Ok(format!("[{}]", serde_json::to_string(content)?))
    }
}

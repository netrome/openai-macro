# LLImp (Large Language Implementation)
## Finally, a solution to Rust's compilation speed problem... by making it WORSE!

Many criticisms have been leveled at Rust compilation speed - CPU cores spinning furiously, `/tmp` filling up with artifacts. But we noticed something: during all this computational chaos, your GPU just sits there idle, probably updating its Instagram feed or mining cryptocurrency for someone else.

**Well, no longer.**

LLImp is a Rust procedural macro that generates trait implementations using Large Language Models. Just write the trait signatures, add our macro, and watch your GPU finally earn its keep by writing your code for you. 

**Why we really built this:** We live in Sweden, and winter is coming. While other developers complain about GPU power consumption, we embrace it. Your AI-generated code literally heats your home. It's environmentally friendly heating that produces working Rust code as a delightful side effect. Who needs a space heater when you can have an AI writing your `impl` blocks while keeping your fingers warm enough to type documentation?

Plus, implementing code this way dramatically simplifies problem solving - instead of actually reading error messages and understanding what went wrong, you can just throw more GPUs at the problem until it works. It's the modern equivalent of solving bugs by turning the computer off and on again, except now you're generating heat while you do it.

**Why you'll love it:**
- 🔒 **Privacy-first**: Runs locally with Ollama - your code never leaves your machine
- 💰 **Free**: No API costs after initial setup
- ⚡ **Actually fast**: After the first generation, everything is cached
- 🎯 **Dead simple**: One macro, empty methods, done
- 🚀 **GPU utilization**: If your name is Jensen Huang, you'll love how this drives GPU sales, funding more leather jackets and increasingly cool kitchen renovations

## Quick Start (5 Minutes)

### 1. Install Ollama (One-time Setup)

```bash
# 1. Install Ollama from https://ollama.com
# 2. Start the server
ollama serve

# 3. Pull a coding model
ollama pull gemma3:latest
```

That's it! No API keys, no cloud accounts, no costs (other than electricity).

### 2. Add LLImp to Your Project

```toml
# Cargo.toml
[dependencies]
llimp = { path = "crates/llimp" }
```

### 3. Write Your First AI-Generated Code

```rust
use llimp::llimp;

trait Calculator {
    fn add(&self, a: i32, b: i32) -> i32;
    fn multiply(&self, a: i32, b: i32) -> i32;
}

struct MyCalculator;

#[llimp(prompt = "Implement basic arithmetic operations")]
impl Calculator for MyCalculator {
    fn add(&self, a: i32, b: i32) -> i32 {
        // Implementation generated by LLM
    }

    fn multiply(&self, a: i32, b: i32) -> i32 {
        // Implementation generated by LLM
    }
}

fn main() {
    let calc = MyCalculator;
    println!("5 + 3 = {}", calc.add(5, 3));        // AI-generated!
    println!("4 * 7 = {}", calc.multiply(4, 7));   // AI-generated!
}
```

### 4. Run Your Code

```bash
export LLM_MODEL=gemma3:latest  # optional, this is the default
cargo run
```

The first run will be slower as LLImp generates implementations. Subsequent runs are fast thanks to caching.

### 5. Try the Examples

LLImp includes several working examples to demonstrate different use cases:

#### Quick Demo (Recommended)
```bash
# Interactive demo - choose your option
make demo

# Or run specific demos:
make test-ollama          # Full Ollama demo with calculator example
```

#### Run Individual Examples
```bash
# Set up Ollama environment (recommended)
export LLM_BASE_URL=http://localhost:11434/v1
unset LLM_API_KEY
export LLM_MODEL=gemma3:latest

# Calculator example (math + text processing with binary)
cargo run -p calculator

# Simple greeting example (library only)
cargo test -p abuse

# Test all generated implementations
cargo test -p calculator
cargo test -p abuse
```

#### Available Examples
- **`calculator`** - Math operations and text processing with runnable main function
- **`abuse`** - Simple greeting trait implementation (library with tests)

**Working Examples:** The `calculator` and `abuse` examples are fully tested and working.

**Note:** Examples use local Ollama by default. First run generates implementations (slower), subsequent runs use cache (fast).

## How It Works

1. **Write trait signatures** with empty method bodies
2. **Add the macro** with optional hints about what you want
3. **Build** - LLImp sends signatures to your local AI model
4. **Get implementations** generated and injected automatically
5. **Subsequent builds** use cached implementations (actually fast!)

## Configuration

### Default: Ollama (Local)

LLImp works out of the box with Ollama running locally:

```bash
ollama serve  # Start Ollama
cargo run     # That's it!
```

No configuration needed - LLImp automatically finds your local Ollama instance.

### Remote Ollama Server

To use a remote Ollama server instead of localhost:

```bash
export OLLAMA_HOST=192.168.1.100  # Your remote server IP or hostname
# or
export OLLAMA_HOST=ollama.example.com  # Remote hostname
cargo run
```

This will connect to `http://192.168.1.100:11434/v1` instead of localhost.

### SSH Tunneling to Remote Ollama

Let's just say... hypothetically... you're too lazy to open up port 11434 on your server running Debian. Hypothetically, you can get a secure tunnel free of charge, and it's superior anyway! Why mess with `ufw allow 11434` and worry about security when SSH does the heavy lifting? Plus, your Debian server probably already has SSH running (because who doesn't), so you're basically getting enterprise-grade tunneling for free:

```bash
# Forward local port 11434 to remote server's Ollama
ssh -L 11434:localhost:11434 user@myserver.local

# Or run in background
ssh -f -N -L 11434:localhost:11434 user@myserver.local
```

Then use LLImp normally (it will connect to localhost:11434 which forwards to the remote server):

```bash
cargo run  # Automatically uses the forwarded connection - like magic, but better
```

**Why SSH tunneling is superior:**
- ✅ No firewall configuration needed
- ✅ Encrypted by default (your AI conversations stay private)
- ✅ Works through NAT, corporate firewalls, and other network nightmares
- ✅ No additional ports to secure or worry about
- ✅ SSH is probably already running anyway
- ✅ Feels like hacking, but it's totally legitimate

For persistent tunneling, add to your `~/.ssh/config`:

```ssh-config
Host ollama-tunnel
    HostName myserver.local
    User your-username
    LocalForward 11434 localhost:11434
    ServerAliveInterval 60
    ServerAliveCountMax 3
```

Then simply run:
```bash
ssh ollama-tunnel
# In another terminal:
cargo run
```

### Override: Cloud APIs

To use Google Gemini or other cloud APIs instead:

```bash
export LLM_MODEL=gemini-2.0-flash-exp
export LLM_API_KEY=your_api_key
export LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai
cargo run
```

Only when **both** `LLM_API_KEY` and `LLM_BASE_URL` are set, cloud APIs are used instead of Ollama.

### Environment Variables

- `LLM_MODEL`: Model to use (default: "gemma3:latest")
- `LLM_API_KEY`: API key for cloud services (not needed for Ollama)
- `LLM_BASE_URL`: Override endpoint URL
- `OLLAMA_HOST`: Ollama server hostname/IP (default: "localhost")
- `LLM_OFFLINE=1`: Use only cached implementations, no network requests

### Macro Parameters

- `prompt` (optional): Additional context or instructions for the AI

## Features

- `no-network`: Compile-time flag to disable network requests and use only cached implementations

## Caching

Generated implementations are automatically cached based on a hash of:
- Trait signatures
- Self type
- Model name
- Prompt hint

The cache is stored in your build output directory (`target/llimp/llimp_cache/`) and persists across builds.

### Performance
- **First build**: Slow (generates implementations)
- **Subsequent builds**: Fast (uses cache)
- **Offline mode**: Use `LLM_OFFLINE=1` to force cache-only operation

## Architecture

### Core Components

1. **`llimp` Proc Macro** (`crates/llimp/`)
   - Parses trait implementations with empty method bodies
   - Generates prompts from trait signatures and user hints
   - Calls LLM APIs to generate implementations
   - Robust response parsing to handle various LLM output formats

2. **Examples**
   - `abuse/` - Simple greeting example
   - `calculator/` - Math operations example
   - `ollama/` - Comprehensive Ollama demonstration

3. **Testing Infrastructure**
   - Simplified Makefile focusing on core use cases
   - Comprehensive test suite for various scenarios
   - CI/CD friendly testing without LLM dependencies

## Workspace Structure

This is a Cargo workspace containing:

- `crates/llimp/`: The main procedural macro crate
- `examples/abuse/`: Example usage demonstrating the macro
- `examples/calculator/`: Calculator implementation example
- `examples/ollama/`: Ollama/local LLM usage example

## Development

### Building

```bash
cargo build
```

### Testing

```bash
# Quick test (no LLM required)
make quick

# Full test with Ollama
export LLM_MODEL=gemma3:latest
make test-ollama

# Test with cloud API
export LLM_MODEL=gemini-2.0-flash-exp
make test-gemini LLM_API_KEY=your_key

# Core library only
cargo test -p llimp
```

### Development Workflow
```bash
make quick      # Fast core tests
make test-ollama # Test with local AI
make ci         # CI-friendly tests
```

### Running in CI

For CI environments, test the core library without LLM dependencies:

```bash
make ci  # Runs core tests only
```

## Advanced Usage

### Custom Models

```bash
export LLM_MODEL=deepseek-coder
```

```rust
#[llimp(prompt = "Use functional programming style")]
impl MyTrait for MyStruct { /* ... */ }
```

### Better Prompts

```rust
#[llimp(prompt = "Implement HTTP client methods. Use reqwest crate. Handle errors properly.")]
impl HttpClient for MyClient {
    /// Fetches JSON data from the given URL
    fn get_json(&self, url: &str) -> Result<serde_json::Value, Box<dyn Error>> {
        // AI generates this
    }
}
```

## Troubleshooting

### "Connection refused"
- Make sure Ollama is running: `ollama serve` (locally) or check remote server
- For remote servers: verify `OLLAMA_HOST` is set correctly and server is accessible
- For SSH access: use port forwarding `ssh -L 11434:localhost:11434 user@myserver.local`
- Check if the model is installed: `ollama list`

### "Model not found"
- Pull the model: `ollama pull gemma3:latest`

### "Compilation slow"
- First run generates code (slow)
- Subsequent runs use cache (fast)
- Use `LLM_OFFLINE=1` to force cache-only

### "Generated code is wrong"
- Improve your prompt with more specific instructions
- Try a different model: `export LLM_MODEL=gemma3:latest`
- Add type hints and documentation to your traits
- Clean cache and retry: `make clean-cache`

### "Examples won't run"
```bash
# For calculator example (local Ollama)
export LLM_BASE_URL=http://localhost:11434/v1
unset LLM_API_KEY
export LLM_MODEL=gemma3:latest
cargo run -p calculator

# For calculator example (remote Ollama)
export OLLAMA_HOST=192.168.1.100  # Your remote server
export LLM_MODEL=gemma3:latest
cargo run -p calculator

# For abuse example (library only)
cargo test -p abuse

# If examples fail to compile, clean cache and retry
make clean-cache
```

### LLM Inconsistency
- **Expected behavior**: AI models sometimes generate invalid code
- **Solution**: Retry by cleaning cache: `make clean-cache && cargo build`
- **Better consistency**: Use cloud APIs with more capable models

## Best Practices

1. **Write good prompts**: Be specific about what you want
   ```rust
   #[llimp(prompt = "Implement using only standard library. Handle edge cases.")]
   ```

2. **Use descriptive trait signatures**:
   ```rust
   trait HttpClient {
       /// Fetches JSON data from the given URL
       fn get_json(&self, url: &str) -> Result<serde_json::Value, Box<dyn Error>>;
   }
   ```

3. **Test generated code**: AI isn't perfect, always test your implementations

4. **Cache for CI**: Use `LLM_OFFLINE=1` in CI/CD to avoid network calls

## Privacy & Security

### Local-First Design
- **Default**: All processing happens locally with Ollama
- **No API keys** required for default operation
- **No data transmission** to external services by default
- **User choice**: Opt-in to cloud APIs when needed

### Security Features
- No hardcoded API keys
- Localhost API key exemption
- Clear separation between local and cloud modes
- Environment variable validation

### API Key Protection
⚠️ **NEVER commit API keys to version control!**

```bash
# ✅ GOOD: Use environment variables
export LLM_API_KEY=your_actual_key
cargo run

# ✅ GOOD: Use .envrc (already gitignored)
echo 'export LLM_API_KEY=your_actual_key' > .envrc

# ❌ BAD: Don't hardcode in source code
const API_KEY: &str = "your_actual_key";  // NEVER DO THIS

# ❌ BAD: Don't put in shell history
cargo run --env LLM_API_KEY=your_actual_key  // Visible in history
```

### Security Best Practices
1. **Use local Ollama when possible** - No API keys needed
2. **Environment variables only** - Never hardcode keys
3. **Check .gitignore** - Ensure `.envrc` and `.env*` files are ignored
4. **Rotate keys regularly** - Change cloud API keys periodically
5. **Use minimal permissions** - API keys should have least privilege needed

## Testing Strategy

### Core Tests (Always Pass)
- `cargo test -p llimp` - Core library functionality
- `make quick` - Fast development feedback
- `make ci` - CI/CD pipeline tests

### LLM Tests (Conditional)
- `make test-ollama` - Local Ollama testing
- `make test-gemini LLM_API_KEY=key` - Cloud API testing

### Philosophy
- **Core functionality** tested without external dependencies
- **LLM integration** tested only when services are available
- **Failure is expected** when LLM services are unavailable (correct behavior)

## Target Users

### Primary: Privacy-Conscious Developers
- Want AI assistance without cloud dependencies
- Prefer local processing for sensitive code
- Value zero-cost operation after setup

### Secondary: Cloud API Users
- Need specific models not available locally
- Willing to use cloud APIs for better performance
- Want seamless override capability

## Getting Help

- 📖 Full docs: This README
- 🔧 Testing: `make help`
- 🦙 Ollama setup: `make help-ollama`
- 🐛 Issues: File an issue on GitHub

## License

Licensed under either of

- Apache License, Version 2.0
- MIT License

at your option.

---

## 🔥 Real User Testimonials

> *"I tested LLImp on my MacBook Air during a particularly cold Stockholm evening. Not only did it generate perfect Rust implementations, but it proved to be an exceptionally efficient lap warmer."*
>
> — **Divan V.**, Senior Engineer & Involuntary Beta Tester of Laptop Heating Solutions

---

**Happy coding with AI! 🚀**

LLImp successfully transforms AI-powered code generation into a **privacy-first, local-by-default** tool. The simplified architecture prioritizes user privacy while maintaining the power of AI-assisted development.

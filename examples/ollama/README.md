# Ollama Example

This example demonstrates how to use `llimp` with [Ollama](https://ollama.com), a local LLM server that provides OpenAI-compatible APIs.

## Why Use Ollama?

- **Privacy**: Run models completely locally, no data sent to external services
- **Cost**: No API costs once models are downloaded
- **Speed**: Direct local inference without network latency
- **Offline**: Works without internet connection
- **Control**: Choose your preferred models and configurations

## Setup

### 1. Install Ollama

Visit [ollama.com](https://ollama.com) and follow the installation instructions for your platform.

### 2. Start Ollama Server

```bash
ollama serve
```

This starts the Ollama server on `http://localhost:11434`.

### 3. Pull a Coding Model

Choose one of these models optimized for code generation:

```bash
# Recommended: Code Llama (good balance of speed and quality)
ollama pull codellama

# Alternative: DeepSeek Coder (excellent for coding tasks)
ollama pull deepseek-coder

# Alternative: Llama 3.1 (general purpose, good for code)
ollama pull llama3.1

# Smaller/faster option: Code Llama 7B
ollama pull codellama:7b
```

### 4. Configure Environment

Set the LLM base URL to point to your local Ollama instance:

```bash
export LLM_BASE_URL=http://localhost:11434/v1
```

No API key is required for local Ollama instances.

### 5. Run the Example

```bash
cargo run --example ollama-example
```

## What This Example Demonstrates

The example showcases three different trait implementations:

1. **FileSystem Operations** - Mock file system with directory and file operations
2. **Data Processing** - CSV parsing, hashing, and basic compression
3. **Advanced Math** - Factorial, Fibonacci, prime checking, and GCD calculations

Each implementation is generated by your local Ollama model based on the trait signatures and provided prompts.

## Customizing the Model

You can change which model is used by updating the `model` parameter in the `#[llimp]` attributes:

```rust
#[llimp(
    model = "deepseek-coder",  // Change this to your preferred model
    prompt = "Your custom prompt here"
)]
impl MyTrait for MyStruct {
    // ...
}
```

## Performance Notes

- **First Run**: Slower as implementations are generated and cached
- **Subsequent Runs**: Much faster using cached implementations
- **Offline Mode**: Use `LLM_OFFLINE=1` to only use cached implementations
- **Model Size**: Larger models (13B, 34B) provide better code quality but are slower

## Troubleshooting

### Ollama Not Running
```
Error: request failed
```
Make sure Ollama is running: `ollama serve`

### Model Not Found
```
Error: model 'codellama' not found
```
Pull the model first: `ollama pull codellama`

### Wrong Endpoint
```
Error: missing OPENAI_API_KEY
```
Make sure you set: `export LLM_BASE_URL=http://localhost:11434/v1`

### Port Conflicts
If port 11434 is busy, you can run Ollama on a different port:
```bash
OLLAMA_HOST=0.0.0.0:11435 ollama serve
export LLM_BASE_URL=http://localhost:11435/v1
```

## Comparing with Cloud APIs

| Aspect | Ollama (Local) | OpenAI/Cloud |
|--------|----------------|--------------|
| Privacy | ✅ Complete | ❌ Data sent externally |
| Cost | ✅ Free after setup | ❌ Pay per token |
| Speed | ⚠️ Depends on hardware | ✅ Consistent |
| Quality | ⚠️ Varies by model | ✅ Generally high |
| Setup | ❌ Requires installation | ✅ Just API key |
| Offline | ✅ Works offline | ❌ Requires internet |

## Next Steps

- Try different models to see which works best for your use case
- Experiment with different prompts to improve code generation quality
- Use the caching system to speed up development iterations
- Consider using larger models for production code generation